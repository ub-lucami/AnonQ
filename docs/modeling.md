Anonimizirana struktura (`GUID`, `week_number`, `weekday`, `time_period`, `generalized_event`) omogoÄa izgradnjo **napovednega modela za prihodnje dogodke**. Ta dokument navaja:

1.  **PriporoÄeno arhitekturo modela**,
2.  **Feature engineering**,
3.  **Primer celotne Python skripte** za uÄenje modela (klasiÄni ML),
4.  **Primer naprednega LSTM/Transformer pristopa** (za sekvenÄno modeliranje uporabnih vzorcev),
5.  **PriporoÄila za evalvacijo**.

***

# Cilj modela

**Napovedati najverjetnejÅ¡i `generalized_event` glede na Äasovne znaÄilnosti.**

To je klasiÄen problem za nadzorovano uÄenje:

*   **X (znaÄilke)**:
    *   `week_number`
    *   `weekday`
    *   `time_period`
    *   `GUID` (opcijsko, Äe ne Å¾eliÅ¡ napovedovati globalno, ampak personalizirano)

*   **y (ciljna spremenljivka)**:
    *   `generalized_event`

***

# Feature engineering (minimalni set za dober model)

## KategoriÄni atributi â†’ integer encoding

*   `GUID`
*   `time_period`
*   `generalized_event`

## NumeriÄni atributi â†’ lahko ostanejo integer:

*   `week_number` (1â€“52)
*   `weekday` (0â€“6)

## Dodatne cikliÄne znaÄilke (priporoÄljivo):

Za tedenski/dnevni ritem uporabiÅ¡ sinus/kosinus:

```python
df["week_sin"] = np.sin(2 * np.pi * df["week_number"] / 52)
df["week_cos"] = np.cos(2 * np.pi * df["week_number"] / 52)

df["weekday_sin"] = np.sin(2 * np.pi * df["weekday"] / 7)
df["weekday_cos"] = np.cos(2 * np.pi * df["weekday"] / 7)
```

***

# Modeli, ki dobro delujejo

### Za hitro produkcijsko uporabo:

*   **XGBoost**
*   **LightGBM**
*   **Random Forest**

â†’ Zelo robustni za tabliÄne podatke, odliÄni za nepopolne vzorce.

### Za sekvenÄno napoved (priporoÄam!):

Za **napovedovanje verjetnosti zaporedij** dogodkov (npr. â€œpo `system_event` obiÄajno sledi `power_event`â€), so moÅ¾nosti:

*   **LSTM**
*   **GRU**
*   **Transformer Encoder** (najboljÅ¡i, a potrebuje veÄ podatkov)

***

# Python skripta (LightGBM model)

Skripta je v datoteki `lightGBM_model.py`. Uporabi se na podatkih `anonymized_events.csv`.

### Zakaj LightGBM?

*   Dela zelo hitro,
*   dobro obvlada kategorikalne featurje,
*   odliÄen na podatkih z neenakomernimi razredi (kar velja za tvoje evente).

***

# SekvenÄno modeliranje (LSTM/Transformer) â€” napoved naslednjega dogodka

ÄŒe Å¾eliÅ¡ **zaporedno napovedovanje**, moraÅ¡ podatke pretvoriti v sekvence:

    GUID1: sys_event â†’ power_event â†’ system_event â†’ ...
    GUID2: measurement â†’ measurement â†’ power_event â†’ ...

Potem zgradiÅ¡ dataset:

```python
sequence_length = 5
# X = (e1,e2,e3,e4,e5)
# y = e6
```

In uporabiÅ¡ PyTorch Keras:

### Mini primer (psevdokoda):

```python
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(num_events, 32),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(num_events, activation="softmax")
])

model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
model.fit(X_seq, y_seq, epochs=20, batch_size=64)
```

Ta pristop deluje **podobno kot jezikovni model (npr. predvidevanje naslednje besede)**.

***

# Evalvacija modela

PriporoÄam naslednje metrike:

### klasifikacijske:

*   **F1-score za vsak event**
*   **macro-F1** (ker imaÅ¡ neuravnoteÅ¾ene razrede)
*   **confusion matrix**

### za sekvence:

*   **topâ€‘k accuracy** (npr. ali je pravi dogodek med top 3)
*   **perplexity** (Äe uporabljaÅ¡ verjetnostni model)

***

# Dodatne moÅ¾nosti

âœ… **Streamlit dashboard**:

*   prikaz podatkov
*   prikaz porazdelitev
*   uÄenje modela
*   prikaz napovedi

ali

âœ… **napredni Transformer model** (kot mini GPT za dogodke)  
ali  
âœ… **celovit Jupyter notebook** s poroÄili, grafi, tuningom, feature engineeringom.

***

**A) klasiÄni ML model**,  
**B) sekvenÄni LSTM/Transformer**,  
**C) personaliziran model za vsak GUID**,  
**D) generalni model za vse uporabnike**?

# âœ… Obseg podatkov za modeliranje?
**26â€¯764 dogodkov** je dovolj velika koliÄina podatkov, za izdelavo *zelo solidnega* napovednega modela. Za napovedovanje dogodkov v pametnem domu / oskrbovanih stanovanjih je **26k dogodkov povsem zadostnih**, ker imajo dogodki jasno sezonsko cikliÄnost (teden, ura dneva, dan v tednu). Glede na obseg razpoloÅ¾ljive baze velja, da je to:

*   dovolj za **klasiÄne ML modele** (LightGBM, XGBoost, RandomForest),
*   dovolj tudi za **sekvenÄne modele (LSTM/GRU)**, *Äe* je na voljo \~200â€“300 dogodkov na GUID,
*   premalo za velik **Transformer (GPTâ€‘podoben)** model â€” vendar je moÅ¾no uporabiti â€œmini-transformerâ€, ki dela z manjÅ¡imi sekvencami.

***

# Predlogi za izvedbo

## Kombiniran pristop:

### 1) **Globalni LightGBM model** (hitro, robustno)

ZaÄnemo s tabliÄnim modelom, ki napove `generalized_event` glede na:

*   `week_number`
*   `weekday`
*   `time_period`
*   `GUID`
*   (optionally) sinus/kosinus transformacije

Ta model bo dal baseline.

### 2) **Personaliziran sekvenÄni model (LSTM)**

Za GUIDâ€‘e z dovolj zaporedij (npr. nad 200 dogodkov) se naj izvede *sekvenÄna napoved naslednjega dogodka*.

â†’ to drastiÄno dvigne natanÄnost tam, kjer uporabniki/senzorji kaÅ¾ejo ponavljajoÄe vzorce.

***

# ğŸ“Š Minimalno Å¡tevilo dogodkov za LSTM?

*   **â‰¥ 10k dogodkov**: dovolj za globalni LSTM
*   **â‰¥ 300 dogodkov na GUID**: dovolj za personaliziran LSTM
*   **< 150 dogodkov na GUID**: sekvenÄni model ne bo bistveno boljÅ¡i od LightGBM

Ker je na voljo **26â€¯764**, bo 99% zadoÅ¡Äalo za globalni LSTM, mogoÄe tudi za perâ€‘GUID modele, Äe posamezni GUIDâ€‘i niso preveÄ redki.

***

# Konkreten predlog cevovoda za podatke

## 1) **Analiza porazdelitve dogodkov po GUID**

Najprej se preveri:

```python
df["GUID"].value_counts().describe()
```

da vemo:

*   koliko GUIDâ€‘ov ima >100 dogodkov,
*   ali imamo dolg rep GUIDâ€‘ov z malo dogodki (kar boÅ¡ odstranil iz LSTM modela).

***

# 2) **Enostaven baseline LightGBM**

(Å¾e na voljo â€” se uporabi.)

***

# 3) âš¡ LSTM model za napovedovanje naslednjega dogodka

**LSTM model** za 26â€¯764 dogodkov je v datoteki `lstm_next_event.py`

***

# ğŸ” Zakaj je to optimalno za tvoj dataset 26k zapisov?

### LightGBM:

*   zelo stabilen za manjÅ¡e in srednje velike datasete
*   razume kategorije in Äasovne cipherje
*   ni prenauÄen pri 26k vzorcih

### LSTM:

*   z 26k dogodki imamo dovolj primerov za zaporedja dolÅ¾ine 10
*   nauÄi se periodiÄnih vzorcev
*   nauÄi se Â»tipiÄnih potiÂ« med dogodki (npr. power â†’ system â†’ supervisory)

### Napredni Transformer:

*   pri 26k dogodkih lahko uporabimo miniâ€‘transformerje (3â€“6 attention headov)
*   vendar ni bistveno boljÅ¡i od LSTM pri tako majhnem obsegu podatkov

***

# Kaj lahko priÄakujemo od rezultatov?

RealistiÄna natanÄnost:

*   **LightGBM**: 60â€“80%
*   **LSTM (global)**: 65â€“82%
*   **LSTM (per GUID)**: lahko 85â€“95% za GUIDâ€‘e z moÄnimi vzorci
*   **Transformer**: podoben LSTM, razen Äe imaÅ¡ >100k dogodkov

***

# MoÅ¾nosti za nadaljevanje:

### **Productionâ€‘ready pipeline**:

*   ingest â†’ feature engineering â†’ modeli â†’ evalvacije â†’ shranjeni modeli â†’ API

### **Dashboard za napovedi** (Streamlit):

*   vizualizacija dogodkov
*   napovedi naslednjega dogodka
*   perâ€‘GUID analitika

### **Miniâ€‘GPT za dogodke**:

*   Transformer, optimiziran za dataset <50k dogodkov
*   deluje kot "next event predictor"

***

ÄŒe Å¾eliÅ¡, lahko pogledava Å¡e natanÄno porazdelitev dogodkov po GUID â€” to je kljuÄno za odloÄitev ali gremo v **perâ€‘GUID LSTM modele**.

